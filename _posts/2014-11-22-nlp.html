---
title:  Natural Language Processing
category:  note
---
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Natural Language Processing</title>
<!-- 2014-12-03 三 20:06 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="robin1001" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="/css/worg.css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Natural Language Processing</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. 参考书籍</a></li>
<li><a href="#sec-2">2. Basic Text Processing</a>
<ul>
<li><a href="#sec-2-1">2.1. Regular Expression</a></li>
<li><a href="#sec-2-2">2.2. Word Tokenization</a></li>
<li><a href="#sec-2-3">2.3. Word Normaliztion &amp; Stemming</a></li>
</ul>
</li>
<li><a href="#sec-3">3. Edit Distance</a></li>
<li><a href="#sec-4">4. Language Modeling</a>
<ul>
<li><a href="#sec-4-1">4.1. 理论 &amp; 应用</a></li>
<li><a href="#sec-4-2">4.2. Perplexity，如何评价一个语言模型</a></li>
<li><a href="#sec-4-3">4.3. 数据稀疏</a></li>
</ul>
</li>
<li><a href="#sec-5">5. Spelling Correction</a>
<ul>
<li><a href="#sec-5-1">5.1. 拼写错误</a></li>
<li><a href="#sec-5-2">5.2. 如何写一个拼写检查器</a></li>
<li><a href="#sec-5-3">5.3. Peter Norvig Corrector</a></li>
</ul>
</li>
<li><a href="#sec-6">6. Text Classification</a>
<ul>
<li><a href="#sec-6-1">6.1. 方法(Naive Bayes, SVM)</a></li>
<li><a href="#sec-6-2">6.2. Naive Bayes</a></li>
</ul>
</li>
<li><a href="#sec-7">7. Sentiment Anylasis</a>
<ul>
<li><a href="#sec-7-1">7.1. 其他名称</a></li>
<li><a href="#sec-7-2">7.2. Steps</a></li>
<li><a href="#sec-7-3">7.3. Naive Bayes in Sentiment</a></li>
<li><a href="#sec-7-4">7.4. 情感词典</a>
<ul>
<li><a href="#sec-7-4-1">7.4.1. 成熟开放的情感词典</a></li>
<li><a href="#sec-7-4-2">7.4.2. Learning Sentiment Lexicons</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-8">8. homework</a></li>
</ul>
</div>
</div>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> 参考书籍</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>朱拉夫斯基和曼宁《语音与语言处理》
</li>
<li>曼宁、舒策、拉哈万《信息检索导论》（2008）
</li>
<li>伯德、克莱因、洛普《使用Python进行自然语言处理》（2009）
</li>
<li>做实验的数据可以在<a href="http://norvig.com/ngrams/">Peter Norvig</a>查找
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Basic Text Processing</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Regular Expression</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>[]: [wW]oodchuck, [0-9], [a-z]
</li>
<li>[^]: not, [\^a-z]
</li>
<li>|: or
</li>
<li>?, *, +, .: times 
</li>
<li>^, &amp;: start &amp; end
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Word Tokenization</h3>
<div class="outline-text-3" id="text-2-2">
</div><ol class="org-ol"><li><a id="sec-2-2-0-1" name="sec-2-2-0-1"></a>shakespeare<br  /><div class="outline-text-5" id="text-2-2-0-1">
<div class="org-src-container">

<pre class="src src-sh">tr -sc <span style="color: #8b2252;">'a-zA-Z'</span> <span style="color: #8b2252;">'\n'</span> &lt; shakespeare.txt | sort | uniq -c | sort -n -r | less
</pre>
</div>
<ul class="org-ul">
<li>将非单词替换为\n
</li>
<li>排序
</li>
<li>去重，并统计出现次数， uniq -c
</li>
<li>按出现次数排序，逆序输出
</li>
<li>如何统计每个单词的重复出现次数 tr -s "\t| " "\n" &lt; word.txt | sort | uniq -c
</li>
</ul>
</div>
</li>
<li><a id="sec-2-2-0-2" name="sec-2-2-0-2"></a>中文的分词<br  /><div class="outline-text-5" id="text-2-2-0-2">
<ul class="org-ul">
<li>max-match
</li>
</ul>
</div>
</li></ol>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> Word Normaliztion &amp; Stemming</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>stem: Porter's algorithm
</li>
<li>affix
</li>
</ul>
</div>

<ol class="org-ol"><li><a id="sec-2-3-0-1" name="sec-2-3-0-1"></a>Sentence Segmentation<br  /><div class="outline-text-5" id="text-2-3-0-1">
<ul class="org-ul">
<li>?, !: 通常没有歧义
</li>
<li>.: 存在歧义，如Dr. 2014.12等。 解决方法，使用决策树判断.是否为句子结束
</li>
</ul>
</div>
</li></ol>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Edit Distance</h2>
<div class="outline-text-2" id="text-3">
<ol class="org-ol">
<li>Levenshtein distance: 替换代价为2,可以理解为先删除再插入
</li>
<li>算法: 动态规划, 通过回溯二维数组pre[i,j] = DOWN, LEFT, DIAG来记录回溯信息
</li>
<li>加权Edit Distance 
<ul class="org-ul">
<li>confusion matrix, a容易被拼写为e;
</li>
<li>物理键盘的布局
</li>
</ul>
</li>
</ol>

<div class="figure">
<p><img src="/img/nlp/confusion_matrix.png" alt="confusion_matrix.png" width="80%" />
</p>
</div>

<div class="figure">
<p><img src="/img/nlp/weighted_edit.png" alt="weighted_edit.png" width="80%" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> Language Modeling</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-sec-4-1" class="outline-3">
<h3 id="sec-4-1"><span class="section-number-3">4.1</span> 理论 &amp; 应用</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>机器翻译, Spell Correction, 语音识别
</li>
<li>一些工具 SRILM, Google N-Gram Release
</li>
<li>a) Markov假设, b)取log避免下溢
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-4-2" class="outline-3">
<h3 id="sec-4-2"><span class="section-number-3">4.2</span> Perplexity，如何评价一个语言模型</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>P[P(W)] = P(W)^1/N, 取对数即为1/NlogP(W)
</li>
<li>Perplexity越小，better model
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-4-3" class="outline-3">
<h3 id="sec-4-3"><span class="section-number-3">4.3</span> 数据稀疏</h3>
<div class="outline-text-3" id="text-4-3">
<div class="center">
<p>
测试数据中出现训练集不存在的语法怎么办？
</p>
</div>
</div>
<ol class="org-ol"><li><a id="sec-4-3-0-1" name="sec-4-3-0-1"></a>Add-One Smoothing<br  /><div class="outline-text-5" id="text-4-3-0-1">
<p>
V个人感觉应为所有uigram即单词个数,不论是几元文法
</p>

<div class="figure">
<p><img src="/img/nlp/add_one.png" alt="add_one.png" width="40%" />
</p>
</div>
<blockquote>
<p>
在V &gt;&gt; c(wi-1)时，即训练语料库中绝大部分n-gram未出现的情况（一般都是如此），Add-one Smoothing后有些“喧宾夺主”的现象，效果不佳。
所以n-gram中一般不使用，但在文本分类中使用
</p>
</blockquote>
</div>
</li>
<li><a id="sec-4-3-0-2" name="sec-4-3-0-2"></a>Good-Turing Estimate(参考数学之美)<br  /><div class="outline-text-5" id="text-4-3-0-2">
<ul class="org-ul">
<li>一般来说，出现一次的词的数量比出现两次的多，出现两次的比出现三次的多。这种定律成为Zipf定律。
</li>
<li>基本思想是利用频率的类别信息对频率进行平滑。调整出现频率为c的n-gram频率
</li>
<li>改进策略就是“对出现次数超过某个阈值的gram，不进行平滑，阈值一般取8~10”
</li>
<li>估计P<sub>0</sub>,就得先统计N<sub>0</sub>
</li>
</ul>
</div>
</li>
<li><a id="sec-4-3-0-3" name="sec-4-3-0-3"></a>Stupid Backoff<br  /><div class="outline-text-5" id="text-4-3-0-3">
<p>
思想很简单找得到直接mle，找不到回退，乘以0.4作为惩罚。
</p>

<p>
如计算bigram如下若该bigram存在，则直接计算score，不存在则回退计算其unigram score， 0.4作为惩罚因子。
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #a020f0;">def</span> <span style="color: #0000ff;">score</span>(<span style="color: #a020f0;">self</span>, sentence):
   <span style="color: #8b2252;">""" Takes a list of strings as argument and returns the log-probability</span>
<span style="color: #8b2252;">   of the sentence using your language model. Use whatever data you</span>
<span style="color: #8b2252;">   computed in train() here."""</span>
   <span style="color: #a0522d;">score</span> = 0.0
   <span style="color: #a0522d;">previous</span> = sentence[0]
   <span style="color: #a020f0;">for</span> token <span style="color: #a020f0;">in</span> sentence[1:]:
           <span style="color: #a0522d;">bicount</span> = <span style="color: #a020f0;">self</span>.bigramCounts[(previous, token)]
           <span style="color: #a0522d;">bi_unicount</span> = <span style="color: #a020f0;">self</span>.unigramCounts[previous]
           <span style="color: #a0522d;">unicount</span> = <span style="color: #a020f0;">self</span>.unigramCounts[token]
           <span style="color: #a020f0;">if</span> bicount &gt; 0:
                   <span style="color: #a0522d;">score</span> += math.log(bicount)
                   <span style="color: #a0522d;">score</span> -= math.log(bi_unicount)
           <span style="color: #a020f0;">else</span>:
                   <span style="color: #a0522d;">score</span> += math.log(0.4)
                   <span style="color: #a0522d;">score</span> += math.log(unicount + 1)
                   <span style="color: #a0522d;">score</span> -= math.log(<span style="color: #a020f0;">self</span>.total + <span style="color: #a020f0;">self</span>.vocab_size)
           <span style="color: #a0522d;">previous</span> = token
   <span style="color: #a020f0;">return</span> score
</pre>
</div>
</div>
</li></ol>
</div>
</div>
<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> Spelling Correction</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-sec-5-1" class="outline-3">
<h3 id="sec-5-1"><span class="section-number-3">5.1</span> 拼写错误</h3>
<div class="outline-text-3" id="text-5-1">
<pre class="example">
据统计，80%的拼写错误编辑距离为1，几乎所有的拼写错误编辑距离小于等于2
Kukich（1992）指出有25%~40%的拼写错误都属于Real-word类型
</pre>
<ul class="org-ul">
<li>拼写错误类型，Non-word和Real-word(虽然拼写错误，但仍出现在词典中)
</li>
<li>w = arg max P(x|w)P(w), 前者为channel(error) model, 后者为language model, 可以使用unigram, bigram and on
</li>
<li>P(x|w)中包含了transpose error，confusion matrix
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-5-2" class="outline-3">
<h3 id="sec-5-2"><span class="section-number-3">5.2</span> 如何写一个拼写检查器</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>根据编辑距离对x生成候选集合{w}, 并计算\(P(x|w)\)
</li>
<li>选择语言模型(unigram | bigram | &#x2026;)
</li>
<li>\(w = arg max P(x|w)P(w)\)
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-5-3" class="outline-3">
<h3 id="sec-5-3"><span class="section-number-3">5.3</span> Peter Norvig Corrector</h3>
<div class="outline-text-3" id="text-5-3">
<p>
参考<a href="http://norvig.com/spell-correct.html">How to Write a Spelling Corrector</a>，这段代码非常简洁优美。
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #a020f0;">import</span> re, collections

<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">words</span>(text): <span style="color: #a020f0;">return</span> re.findall(<span style="color: #8b2252;">'[a-z]+'</span>, text.lower()) 

<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">train</span>(features):
        <span style="color: #a0522d;">model</span> = collections.defaultdict(<span style="color: #a020f0;">lambda</span>: 1)
        <span style="color: #a020f0;">for</span> f <span style="color: #a020f0;">in</span> features:
                <span style="color: #a0522d;">model</span>[f] += 1
        <span style="color: #a020f0;">return</span> model

<span style="color: #a0522d;">NWORDS</span> = train(words(<span style="color: #483d8b;">file</span>(<span style="color: #8b2252;">'big.txt'</span>).read()))

<span style="color: #a0522d;">alphabet</span> = <span style="color: #8b2252;">'abcdefghijklmnopqrstuvwxyz'</span>

<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">edits1</span>(word):
        <span style="color: #a0522d;">n</span> = <span style="color: #483d8b;">len</span>(word)
        <span style="color: #a020f0;">return</span> <span style="color: #483d8b;">set</span>([word[0:i]+word[i+1:] <span style="color: #a020f0;">for</span> i <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(n)] +                     <span style="color: #b22222;"># </span><span style="color: #b22222;">deletion</span>
                           [word[0:i]+word[i+1]+word[i]+word[i+2:] <span style="color: #a020f0;">for</span> i <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(n-1)] + <span style="color: #b22222;"># </span><span style="color: #b22222;">transposition</span>
                           [word[0:i]+c+word[i+1:] <span style="color: #a020f0;">for</span> i <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(n) <span style="color: #a020f0;">for</span> c <span style="color: #a020f0;">in</span> alphabet] + <span style="color: #b22222;"># </span><span style="color: #b22222;">alteration</span>
                           [word[0:i]+c+word[i:] <span style="color: #a020f0;">for</span> i <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(n+1) <span style="color: #a020f0;">for</span> c <span style="color: #a020f0;">in</span> alphabet])  <span style="color: #b22222;"># </span><span style="color: #b22222;">insertion</span>

<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">known_edits2</span>(word):
        <span style="color: #a020f0;">return</span> <span style="color: #483d8b;">set</span>(e2 <span style="color: #a020f0;">for</span> e1 <span style="color: #a020f0;">in</span> edits1(word) <span style="color: #a020f0;">for</span> e2 <span style="color: #a020f0;">in</span> edits1(e1) <span style="color: #a020f0;">if</span> e2 <span style="color: #a020f0;">in</span> NWORDS)

<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">known</span>(words): <span style="color: #a020f0;">return</span> <span style="color: #483d8b;">set</span>(w <span style="color: #a020f0;">for</span> w <span style="color: #a020f0;">in</span> words <span style="color: #a020f0;">if</span> w <span style="color: #a020f0;">in</span> NWORDS)

<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">correct</span>(word):
        <span style="color: #a0522d;">candidates</span> = known([word]) <span style="color: #a020f0;">or</span> known(edits1(word)) <span style="color: #a020f0;">or</span> known_edits2(word) <span style="color: #a020f0;">or</span> [word]
        <span style="color: #a020f0;">return</span> <span style="color: #483d8b;">max</span>(candidates, key=<span style="color: #a020f0;">lambda</span> w: NWORDS[w])

<span style="color: #a020f0;">if</span> <span style="color: #483d8b;">__name__</span> == <span style="color: #8b2252;">'__main__'</span>:
        <span style="color: #a020f0;">while</span> <span style="color: #008b8b;">True</span>:
                <span style="color: #a020f0;">print</span> correct(<span style="color: #483d8b;">raw_input</span>(<span style="color: #8b2252;">"&gt;"</span>))
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> Text Classification</h2>
<div class="outline-text-2" id="text-6">
</div><div id="outline-container-sec-6-1" class="outline-3">
<h3 id="sec-6-1"><span class="section-number-3">6.1</span> 方法(Naive Bayes, SVM)</h3>
</div>
<div id="outline-container-sec-6-2" class="outline-3">
<h3 id="sec-6-2"><span class="section-number-3">6.2</span> Naive Bayes</h3>
<div class="outline-text-3" id="text-6-2">
<p>
基本和原来机器学习时内容一致，几个在实践时需要关注的问题
</p>
<ul class="org-ul">
<li>取log，避免下溢
</li>
<li>Add-One Smoothing
</li>
<li>频率为统计频率，not boolean freqency
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7"><span class="section-number-2">7</span> Sentiment Anylasis</h2>
<div class="outline-text-2" id="text-7">
</div><div id="outline-container-sec-7-1" class="outline-3">
<h3 id="sec-7-1"><span class="section-number-3">7.1</span> 其他名称</h3>
<div class="outline-text-3" id="text-7-1">
<p>
Opnion Extraction, Opnion Mining, Sentiment Mining, Subjectivity Anylasis
</p>
</div>
</div>
<div id="outline-container-sec-7-2" class="outline-3">
<h3 id="sec-7-2"><span class="section-number-3">7.2</span> Steps</h3>
<div class="outline-text-3" id="text-7-2">
<ul class="org-ul">
<li>tokenize
</li>
<li>Feature Extraction
</li>
<li>分类器，这里分类器可以使用Naive Bayes, Svm, MatEnt等，后两者效果要好。
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-7-3" class="outline-3">
<h3 id="sec-7-3"><span class="section-number-3">7.3</span> Naive Bayes in Sentiment</h3>
<div class="outline-text-3" id="text-7-3">
<p>
主题仅与词出现有关系，和出现次数关系不大。所以这里使用Boolean Multinomial Naive Bayes(Mutivariate Benoulli Naive Bayes),
即贝努利频率，相应的Add-one方法更改为Add-delta
$$P(w|c) = \frac{n_{k}+\alpha} {n+\alpha|V|}$$
</p>
</div>
</div>
<div id="outline-container-sec-7-4" class="outline-3">
<h3 id="sec-7-4"><span class="section-number-3">7.4</span> 情感词典</h3>
<div class="outline-text-3" id="text-7-4">
</div><div id="outline-container-sec-7-4-1" class="outline-4">
<h4 id="sec-7-4-1"><span class="section-number-4">7.4.1</span> 成熟开放的情感词典</h4>
<div class="outline-text-4" id="text-7-4-1">
<ul class="org-ul">
<li>GI（The General Inquirer）
</li>
<li>LIWC (Linguistic Inquiry and Word Count)
</li>
<li>&#x2026;
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-7-4-2" class="outline-4">
<h4 id="sec-7-4-2"><span class="section-number-4">7.4.2</span> Learning Sentiment Lexicons</h4>
<div class="outline-text-4" id="text-7-4-2">
<ul class="org-ul">
<li>利用and和but连接词的词性关系(这个想法很棒)
</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-sec-8" class="outline-2">
<h2 id="sec-8"><span class="section-number-2">8</span> homework</h2>
<div class="outline-text-2" id="text-8">
<ol class="org-ol">
<li>regular expression
</li>
<li>autocorrect(edit distance, language model)
</li>
<li>test
</li>
</ol>
<p class="verse">
;;不同点有两处，一是形参用逗号直接求值，形式体则用逗号和@ 即 “,@” 去掉对形式体求值后所得到的表达式最外层列表的括号，将这个表达式嵌入到最外围列表的最后面；二是形参要做一系列处理，而形式体则直接求值嵌入，不做任何变化，这是因为这个形式体本来就是新宏的处理语句，定义宏只需要照搬即可，不需要也不应该做其他变化。<br  />
</p>
<blockquote>
<p>
缩进区块
</p>
</blockquote>
<div class="center">
<p>
Everything should be made as simple as possible, but not any simpler
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: robin1001</p>
<p class="date">Created: 2014-12-03 三 20:06</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.4.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
